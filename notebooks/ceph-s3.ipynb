{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FR** - Exemples d'interaction avec un service Ceph de type stockage objet (S3)\n",
    "<hr> \n",
    "\n",
    "**EN** - Examples of how to interact with a Ceph object storage (S3) service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import getpass # Used to input secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "access_key = getpass.getpass(prompt='Access key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "secret_access_key = getpass.getpass(prompt='Secret access key: ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "\n",
    "s3_client = session.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_access_key,\n",
    "    endpoint_url='https://142.98.30.204:8080', # Load balancer url\n",
    "    use_ssl=False,\n",
    "    verify=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client.create_bucket(Bucket=\"fst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client.list_objects(Bucket=\"fst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put file in bucket\n",
    "data = open('your_file_path', 'rb')\n",
    "s3_client.put_object(Bucket=\"your_bucket\", Key='filename_you_want', Body=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through objects listed until no more pagination (IsTruncated: False)\n",
    "bucket_name = 'fst'\n",
    "objects = []\n",
    "\n",
    "response = s3_client.list_objects(Bucket=bucket_name)\n",
    "\n",
    "while True:\n",
    "    for obj in response.get('Contents', []):\n",
    "        objects.append(obj)\n",
    "\n",
    "    if response.get('IsTruncated', False):\n",
    "        # If IsTruncated is True, there are more objects to retrieve.\n",
    "        marker = response.get('NextMarker', None)\n",
    "        response = s3_client.list_objects(Bucket=bucket_name, Marker=marker)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Now, the 'objects' list contains all objects in the bucket.\n",
    "\n",
    "if not objects:\n",
    "    print(\"The list is empty.\")\n",
    "else:\n",
    "    for obj in objects:\n",
    "        print(f\"Object: {obj['Key']}, Size: {obj['Size']} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete all objects in <bucket_name>\n",
    "\n",
    "while True:\n",
    "    # List objects in the bucket\n",
    "    response = s3_client.list_objects(Bucket=bucket_name)\n",
    "\n",
    "    # Create a list of objects to delete\n",
    "    objects_to_delete = [{'Key': obj['Key']} for obj in response.get('Contents', [])]\n",
    "    print(\"Objects to delete list = \" + str(objects_to_delete))\n",
    "\n",
    "    # Delete the objects\n",
    "    if not(objects_to_delete):\n",
    "        print(\"Empty bucket.  No objects deleted.\")\n",
    "        break\n",
    "    else:\n",
    "        delete_response = s3_client.delete_objects(\n",
    "            Bucket=bucket_name,\n",
    "            Delete={'Objects': objects_to_delete}\n",
    "        )\n",
    "\n",
    "        # Check the response for errors\n",
    "        if 'Errors' in delete_response:\n",
    "            for error in delete_response['Errors']:\n",
    "                print(f\"Error deleting object {error['Key']}: {error['Message']}\")\n",
    "\n",
    "    # If there are more objects to list, continue with the next marker\n",
    "    if response.get('IsTruncated', False):\n",
    "        marker = response.get('NextMarker', None)\n",
    "        response = s3_client.list_objects(Bucket=bucket_name, Marker=marker)\n",
    "    else:\n",
    "        print(\"All objects deleted successfully.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T15:48:46.762393Z",
     "iopub.status.busy": "2023-11-08T15:48:46.762110Z",
     "iopub.status.idle": "2023-11-08T15:48:46.764559Z",
     "shell.execute_reply": "2023-11-08T15:48:46.764173Z",
     "shell.execute_reply.started": "2023-11-08T15:48:46.762377Z"
    },
    "tags": []
   },
   "source": [
    "## Cut up a path to retain filename.ext\n",
    "Useful to upload files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_key(fullpath):\n",
    "   \"\"\"\n",
    "   This function takes a full file path as input and returns the file name with its extension.\n",
    "\n",
    "   Parameters:\n",
    "   fullpath (str): The full path to the file.\n",
    "\n",
    "   Returns:\n",
    "   str: The file name with its extension.\n",
    "   \"\"\"\n",
    "   # Split the fullpath once and store the result\n",
    "   path_parts = fullpath.split('/')\n",
    "   filename_with_extension = path_parts[-1]\n",
    "\n",
    "   # Split the filename and extension into separate variables\n",
    "   filename, extension = filename_with_extension.split('.')\n",
    "\n",
    "   # Return the filename and extension\n",
    "   return filename + \".\" + extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_key(\"my/path/and/filename.rs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big file upload\n",
    "- get upload_id\n",
    "- chunk file (5GB is max chunk size generally)\n",
    "- finish multipart upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the bucket and object key\n",
    "bucket_name = 'your_bucket'\n",
    "file_path = 'input_file'\n",
    "object_key = get_key(file_path)\n",
    "\n",
    "# Initiate Multipart Upload\n",
    "response = s3_client.create_multipart_upload(Bucket=bucket_name, Key=object_key)\n",
    "upload_id = response['UploadId'] # upload_id needed further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the large file into smaller parts\n",
    "chunk_size = 5 * 1024 * 1024 * 1024 # 5 GB\n",
    "parts = []\n",
    "file = 'your/full/path/filename.ext'\n",
    "# file = output_file_6gb\n",
    "with open(file, 'rb') as f:\n",
    "    i = 1\n",
    "    while True:\n",
    "        data = f.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        part = s3_client.upload_part(\n",
    "            Bucket=bucket_name,\n",
    "            Key=get_key(file),\n",
    "            PartNumber=i,\n",
    "            UploadId=upload_id,\n",
    "            Body=data\n",
    "        )\n",
    "        parts.append({'PartNumber': i, 'ETag': part['ETag']})\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Complete the multipart upload\n",
    "s3_client.complete_multipart_upload(\n",
    "    Bucket=bucket_name,\n",
    "    Key=object_key,\n",
    "    MultipartUpload={'Parts': parts},\n",
    "    UploadId=upload_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Zarr files really are a collection of files and directories.  The example below therefore is more like a `how to` for uploading a directory. <br> <br>\n",
    "The specific case illustrated here is that of a MinIO service that has \"certificate issues\".  The common use case is a MinIO test set up using a self-signed certificate but this example uses the internal DNS name of one of the pods where the MinIO service is running (and its exposed `NodePort`), though this is not shown explicitly thanks to `getpass`. Of course, an external IP is necessary for production use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given `client`, `bucket_name` defined above and\n",
    "# a Zarr file (a directory that has the name of the dataset and contains the series of files and sub-directories that make up the zarr \"file\")\n",
    "\n",
    "def upload_zarr_directory(client, bucket_name, local_directory):\n",
    "    try:\n",
    "        # Validate the arguments\n",
    "        assert isinstance(client, Minio), \"client must be an instance of Minio\"\n",
    "        assert isinstance(bucket_name, str), \"bucket_name must be a string\"\n",
    "        assert isinstance(local_directory, str), \"local_directory must be a string\"\n",
    "\n",
    "        # Check if the bucket exists\n",
    "        if not client.bucket_exists(bucket_name):\n",
    "            raise ValueError(\"Bucket '{}' does not exist on the client\".format(bucket_name))\n",
    "        \n",
    "        # Check if the zarr_file exists\n",
    "\n",
    "        if not pathlib.Path(zarr_file).is_file():\n",
    "            raise ValueError(f\"{zarr_file} is not a valid local file.\")\n",
    "\n",
    "        for file_path in pathlib.Path(local_directory).glob('**/*'):\n",
    "            if file_path.is_file():\n",
    "                object_name = str(pathlib.Path(zarr_filename) / file_path.relative_to(local_directory))\n",
    "                client.fput_object(bucket_name, object_name, str(file_path))\n",
    "\n",
    "    except (AssertionError, ValueError) as e:\n",
    "        raise ValueError(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Synchronize local directory with a bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be interesting for users to have a copy of a specific folder in a bucket, as a form of backup or as a way to have access to resources through-the-web (TTW).  The MinIO client (`mc`) allows one to \"[mirror](https://min.io/docs/minio/linux/reference/minio-mc/mc-mirror.html)\" a local directory to a bucket but it is a one-way process and it obviously requires `mc` to be installed.  Although it is fairly easy to do so -- just [download one file](https://min.io/docs/minio/linux/reference/minio-mc.html#install-mc), make it executable and run it -- there is a more comprehensive solution : [FUSE](https://en.wikipedia.org/wiki/Filesystem_in_Userspace), more specifically in our case [s3fs](https://linuxbeast.com/aws-operations/how-to-install-s3fs-and-mount-an-s3-bucket-on-ubuntu-20-04/).<br><br>\n",
    "As mentioned in the latter url : \n",
    "\n",
    "```\n",
    "The use case for S3fs is for anyone who needs to access Amazon S3 storage in a more traditional file system interface. This can be especially useful for backing up data, archiving files, or sharing data between different systems. With S3fs, you can interact with S3 as if it were a local file system, making it much easier to automate data transfer and retrieval processes. S3fs is also useful for organizations that use Amazon S3 as their primary storage solution, as it provides a more seamless way to access and manage the data stored there.\n",
    "```\n",
    "\n",
    "> NOTE : MinIO is an implementation of AWS's S3.  As such, software like `s3fs`, which are designed primarily for working with AWS S3, works with other S3 implementations.  The FUSE system can also be used to mount a local directory on Azure object storage, but one must use Microsoft's [blobfuse2](https://learn.microsoft.com/en-us/azure/storage/blobs/blobfuse2-what-is) to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is shown in a notebook cell but should be carried out diretly on the commandline (shown by the prompt sign `$`)\n",
    "# With `s3fs` previously installed on your system\n",
    "# $ s3fs destination_bucket_name local_directory -o passwd_file=path_to_your_creds_file -o url=url_to_your_minio_service -o use_path_request_style -o ssl_verify_hostname=0 -o no_check_certificate\n",
    "# Command to verify your directory is indeed mounted onto a MinIO bucket :\n",
    "# $ mount | grep s3fs\n",
    "# s3fs on `local_directory` type fuse.s3fs (rw,nosuid,nodev,relatime,user_id=61144,group_id=61144)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the options (`-o`).\n",
    "- `passwd_file` must contain one line structured like so : access_key:Secret_access_key.  That is both credential items are separated by a colon (`:`)\n",
    "- `use_path_request_style` apparently needed for MinIO\n",
    "- `ssl_verify_hostname` and `no_check_certificate` : needed to bypass SSL issues.\n",
    "\n",
    "> Note : SSL errors will most often go unnoticed.  You simply won't be able to mount directories with no indication of failure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMDS310",
   "language": "python",
   "name": "cmds310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
